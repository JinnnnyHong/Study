{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxxMdYwQFbzB9lzbVu+lSG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JinnnnyHong/Study/blob/main/Model_st.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 딥 러닝 기법 - RNN\n",
        "\n",
        "보통 RNN 모델에 입력하기 위해 시퀀스 데이터를 고정된 길이의 벡터로 변환하는 과정 필요\n",
        "\n",
        "\n",
        "1) padding : 시퀀스 데이터를 고정된 길이로 만들기 위해 부족한 부분을 0으로 채움. 하지만 데이터가 길이가 매우 다양한 경우 패딩으로 인해 메모리 사용량이 증가하고, 모델 성능이 저하될 수 있음\n",
        "\n",
        "\n",
        "2) Truncation : 시퀀스 데이터의 길이를 고정된 길이로 자르는 방법. 이 방법도 패딩과 같이 데이터의 일부가 잘려나가게 되는 문제가 발생할 수 있음\n",
        "\n",
        "\n",
        "3) embedding : 시퀀스 데이터를 고정된 길이의 벡터로 변환하는 방법 중 하나로, 각 시간 단계마다 특정 차원의 벡터로 표현. 이 방법은 패딩과 Truncation과 달리 정보의 손실이 적고, 모델의 성능을 향상시킬 수 있음"
      ],
      "metadata": {
        "id": "zaT3l0eu9_NM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlFAfuxLRc45"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet101\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "class RRNN(nn.Module):\n",
        "    def __init__(self, day_length, hidden_size, day_hidden_size):\n",
        "        super(RRNN, self).__init__()\n",
        "        self.lstm1 = nn.LSTM(4, day_hidden_size, num_layers=1, bias=True, batch_first=True,\n",
        "                            dropout=0, bidirectional=True)\n",
        "        self.lstm2 = nn.LSTM(44+2*day_hidden_size, hidden_size, num_layers=1, bias=True, batch_first=True,\n",
        "                            dropout=0, bidirectional=True)\n",
        "        self.fc = nn.Linear(2*hidden_size, 2)\n",
        "        self.day_length = day_length\n",
        "        self.day_hidden_size = day_hidden_size\n",
        "        self.hidden_size = hidden_size\n",
        "    def init_hidden(self, batch_size, hidden_size):\n",
        "        return (Variable(torch.randn(2, batch_size, hidden_size)).cuda(),\n",
        "                Variable(torch.randn(2, batch_size, hidden_size)).cuda())\n",
        "\n",
        "    def forward(self, x, d_):\n",
        "        input_storage = []\n",
        "        for i in range(x.shape[1]):\n",
        "            self.hidden = self.init_hidden(x.shape[0], self.day_hidden_size)\n",
        "            x_day = x[:, i, :, :]\n",
        "            x_day = torch.transpose(x_day, 1,2)\n",
        "            x_day, h = self.lstm1(x_day, self.hidden)\n",
        "            x_day = torch.split(x_day, self.day_hidden_size, dim=2)\n",
        "            x_day = torch.cat([x_day[0][:, -1, :], x_day[1][:, 0, :]], dim=1)\n",
        "            input_storage.append(x_day)\n",
        "        f = torch.stack(input_storage, dim=1).squeeze(dim=2)\n",
        "        mix_f = torch.cat([f, d_], dim=2)\n",
        "        self.hidden = self.init_hidden(mix_f.shape[0], self.hidden_size)\n",
        "        xx, h = self.lstm2(mix_f, self.hidden)\n",
        "        xx = torch.split(xx, self.hidden_size, dim=2)\n",
        "        xx = torch.cat([xx[0][:, -1, :], xx[1][:, 0, :]], dim=1)\n",
        "        out = self.fc(xx)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Random Forest\n",
        "\n",
        "의사결정나무의 단점인 과적합을 보완할 수 있으며, 변수의 중요도를 계산할 수 있음\n",
        "\n",
        "랜덤 포레스트 모델은 입력 변수의 스케일에 영향을 받지 않기 때문에, 정규화를 하지 않아도 된다. \n",
        "\n",
        "하지만, 이전과 마찬가지로 문자열과 같은 비숫자형 데이터를 숫자형 데이터로 변환해야 한다. 이를 위해 One-hot 인코딩, 라벨 인코딩 등의 방법을 사용할 수 있다."
      ],
      "metadata": {
        "id": "LpLtj20Q-K-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 데이터 로드\n",
        "iris = load_iris()\n",
        "\n",
        "# 입력 변수와 타겟 변수 분리\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 학습 데이터와 검증 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 랜덤 포레스트 모델 생성\n",
        "rfc = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
        "\n",
        "# 모델 학습\n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "# 검증 데이터에 대한 예측\n",
        "y_pred = rfc.predict(X_test)\n",
        "\n",
        "# 모델 성능 평가\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GfTFb9Q-ZGe",
        "outputId": "3537ddba-d5de-4649-f04c-3b0d64aeb047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 로지스틱 회귀분석\n",
        "이진 분류 문제를 해결하기 위한 분석, 데이터는 숫자형이어야 하며, **문자형일시 숫자형 데이터로 변환**하는 과정이 필요함"
      ],
      "metadata": {
        "id": "6ZDVkDzB-wSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 데이터 로드\n",
        "diabetes = load_diabetes()\n",
        "\n",
        "# 입력 변수와 타겟 변수 분리\n",
        "X = diabetes.data\n",
        "y = diabetes.target\n",
        "\n",
        "# 학습 데이터와 검증 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 선형 회귀 모델 생성\n",
        "linreg = LinearRegression()\n",
        "\n",
        "# 모델 학습\n",
        "linreg.fit(X_train, y_train)\n",
        "\n",
        "# 검증 데이터에 대한 예측\n",
        "y_pred = linreg.predict(X_test)\n",
        "\n",
        "# 모델 성능 평가\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"MSE: %.2f\" % mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIAGzdVC-ZZ7",
        "outputId": "df36eec3-4c33-4c02-8ab1-51f5648f323b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 2900.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "랜덤 포레스트 - 시계열 데이터 적용 예시"
      ],
      "metadata": {
        "id": "ErD8ESyucmYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 데이터 불러오기\n",
        "data = pd.read_csv('data.csv', index_col=0, parse_dates=True)\n",
        "\n",
        "# 특징(feature)과 타겟(target) 변수 설정\n",
        "features = data.iloc[:-1]\n",
        "target = data.iloc[1:]\n",
        "\n",
        "# 훈련용과 테스트용 데이터 분리\n",
        "train_size = int(len(features) * 0.7)\n",
        "train_features = features[:train_size]\n",
        "train_target = target[:train_size]\n",
        "test_features = features[train_size:]\n",
        "test_target = target[train_size:]\n",
        "\n",
        "# 랜덤 포레스트 모델 생성 및 학습\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(train_features, train_target)\n",
        "\n",
        "# 예측\n",
        "predictions = rf.predict(test_features)\n",
        "\n",
        "# 평가\n",
        "mse = mean_squared_error(test_target, predictions)\n",
        "print('Mean Squared Error:', mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "eIFklEyi-_dT",
        "outputId": "a789988a-caf0-42a1-ac0b-9bd217904719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d008fcc7c8cb>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 데이터 불러오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 특징(feature)과 타겟(target) 변수 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
          ]
        }
      ]
    }
  ]
}